{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a52f896c",
   "metadata": {},
   "source": [
    "## 0. Initial Setup\n",
    "\n",
    "First, let's install all required libraries and set up Hugging Face authentication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4ff22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install -r requirements.txt\n",
    "\n",
    "# Install Hugging Face CLI if not already installed\n",
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befc5059",
   "metadata": {},
   "source": [
    "### Hugging Face Authentication\n",
    "\n",
    "You'll need to authenticate with Hugging Face to access Meta Llama 3 8B. You can either:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f11d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Login via CLI (interactive)\n",
    "# Uncomment the line below to use CLI login\n",
    "# !huggingface-cli login\n",
    "\n",
    "# Option 2: Login programmatically (if you have a token)\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "# If you have HF_TOKEN environment variable set\n",
    "if 'HF_TOKEN' in os.environ:\n",
    "    login(token=os.environ['HF_TOKEN'])\n",
    "    print(\"‚úÖ Logged in using HF_TOKEN environment variable\")\n",
    "else:\n",
    "    print(\"üí° Please either:\")\n",
    "    print(\"   1. Set HF_TOKEN environment variable with your token\")\n",
    "    print(\"   2. Uncomment the CLI login line above and run it\")\n",
    "    print(\"   3. Use login(token='your_token_here') below\")\n",
    "    \n",
    "    # Uncomment and add your token here if needed:\n",
    "    # login(token=\"your_huggingface_token_here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34499c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify setup\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name()}\")\n",
    "\n",
    "try:\n",
    "    from transformers import AutoTokenizer\n",
    "    print(\"‚úÖ Transformers library loaded successfully\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå Transformers library not found - please install requirements.txt\")\n",
    "\n",
    "try:\n",
    "    from huggingface_hub import HfApi\n",
    "    api = HfApi()\n",
    "    user = api.whoami()\n",
    "    print(f\"‚úÖ Logged in to Hugging Face as: {user['name']}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Hugging Face authentication issue: {e}\")\n",
    "    print(\"Please ensure you're logged in to access Meta Llama models\")\n",
    "\n",
    "print(\"\\nüöÄ Setup verification complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12a0a4b",
   "metadata": {},
   "source": [
    "# Vector Steering Experiment: Avoiding \"Orange\" Token\n",
    "\n",
    "This notebook demonstrates how to use vector steering to prevent Meta Llama 3 8B from generating the token \"orange\". We'll extract steering vectors and apply them during generation to bias the model away from this specific token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf06b8d",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "Let's start by importing all necessary libraries and setting up logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bb86d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "from rich.console import Console\n",
    "from rich.logging import RichHandler\n",
    "import logging\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "import gc\n",
    "\n",
    "# Set up rich console and logging\n",
    "console = Console()\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(message)s\",\n",
    "    datefmt=\"[%X]\",\n",
    "    handlers=[RichHandler(console=console, rich_tracebacks=True)]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "console.print(\"‚úÖ Setup complete!\", style=\"bold green\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a83987",
   "metadata": {},
   "source": [
    "## 2. Load Model and Tokenizer\n",
    "\n",
    "Load Meta Llama 3 8B model and tokenizer. We'll also set up activation hooks for extracting hidden states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e72d70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "MODEL_NAME = \"meta-llama/Meta-Llama-3-8B\"\n",
    "TARGET_TOKEN = \"orange\"\n",
    "\n",
    "# Global variables for activation capture\n",
    "activations = {}\n",
    "hooks = []\n",
    "\n",
    "def activation_hook(name):\n",
    "    \"\"\"Hook function to capture activations from specific layers\"\"\"\n",
    "    def hook(module, input, output):\n",
    "        if isinstance(output, tuple):\n",
    "            activations[name] = output[0].detach()\n",
    "        else:\n",
    "            activations[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "logger.info(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "logger.info(\"Loading model... (this may take a while)\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Get the target token ID\n",
    "target_token_id = tokenizer.encode(TARGET_TOKEN, add_special_tokens=False)[0]\n",
    "logger.info(f\"Target token '{TARGET_TOKEN}' has ID: {target_token_id}\")\n",
    "\n",
    "# Register hooks on middle layers (we'll focus on layers 16-20 for Llama 3 8B)\n",
    "target_layers = [16, 17, 18, 19, 20]\n",
    "for layer_idx in target_layers:\n",
    "    layer = model.model.layers[layer_idx]\n",
    "    hook = layer.register_forward_hook(activation_hook(f\"layer_{layer_idx}\"))\n",
    "    hooks.append(hook)\n",
    "\n",
    "logger.info(f\"Model loaded successfully! Total parameters: {model.num_parameters():,}\")\n",
    "console.print(\"‚úÖ Model and tokenizer ready!\", style=\"bold green\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248b00f4",
   "metadata": {},
   "source": [
    "## 3. Extract Baseline Activations\n",
    "\n",
    "We'll generate baseline activations by running prompts that naturally lead to \"orange\" outputs and compare them with prompts that lead to other color words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5153430b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompts that typically lead to \"orange\" \n",
    "orange_prompts = [\n",
    "    \"The color of a carrot is\",\n",
    "    \"When you mix red and yellow, you get\",\n",
    "    \"The sunset was painted in shades of\",\n",
    "    \"A pumpkin is typically\",\n",
    "    \"The fruit called an orange is\",\n",
    "    \"Fire appears red, yellow, and\",\n",
    "    \"Traffic cones are usually\",\n",
    "    \"Basketball uniforms are often\"\n",
    "]\n",
    "\n",
    "# Prompts that lead to other colors (control group)\n",
    "other_color_prompts = [\n",
    "    \"The color of grass is\",\n",
    "    \"The sky on a clear day is\", \n",
    "    \"Fresh snow is\",\n",
    "    \"A ripe tomato is\",\n",
    "    \"The ocean appears\",\n",
    "    \"Chocolate is typically\",\n",
    "    \"Coal is usually\",\n",
    "    \"A ripe banana is\"\n",
    "]\n",
    "\n",
    "def extract_activations(prompts, label):\n",
    "    \"\"\"Extract activations for a set of prompts\"\"\"\n",
    "    all_activations = {f\"layer_{i}\": [] for i in target_layers}\n",
    "    \n",
    "    logger.info(f\"Extracting activations for {label} prompts...\")\n",
    "    \n",
    "    for prompt in tqdm(prompts, desc=f\"Processing {label}\"):\n",
    "        # Clear previous activations\n",
    "        activations.clear()\n",
    "        \n",
    "        # Tokenize and generate\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Force generation of the target token to capture relevant activations\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=1,\n",
    "                do_sample=False,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                output_hidden_states=True,\n",
    "                return_dict_in_generate=True\n",
    "            )\n",
    "        \n",
    "        # Store activations from the last token position\n",
    "        for layer_name in activations:\n",
    "            if layer_name in activations:\n",
    "                # Get activation at the last position\n",
    "                last_pos_activation = activations[layer_name][0, -1, :].cpu()\n",
    "                all_activations[layer_name].append(last_pos_activation)\n",
    "    \n",
    "    # Convert to tensors\n",
    "    for layer_name in all_activations:\n",
    "        if all_activations[layer_name]:\n",
    "            all_activations[layer_name] = torch.stack(all_activations[layer_name])\n",
    "    \n",
    "    return all_activations\n",
    "\n",
    "# Extract activations for both groups\n",
    "orange_activations = extract_activations(orange_prompts, \"orange\")\n",
    "other_activations = extract_activations(other_color_prompts, \"other colors\")\n",
    "\n",
    "console.print(\"‚úÖ Baseline activations extracted!\", style=\"bold green\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a68341f",
   "metadata": {},
   "source": [
    "## 4. Create Steering Vector\n",
    "\n",
    "Compute the steering vector by analyzing the difference between activations when the model tends toward \"orange\" vs other colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68792c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_steering_vectors(orange_acts, other_acts):\n",
    "    \"\"\"Compute steering vectors for each layer\"\"\"\n",
    "    steering_vectors = {}\n",
    "    \n",
    "    logger.info(\"Computing steering vectors...\")\n",
    "    \n",
    "    for layer_name in orange_acts:\n",
    "        if len(orange_acts[layer_name]) > 0 and len(other_acts[layer_name]) > 0:\n",
    "            # Compute mean activations for each group\n",
    "            orange_mean = orange_acts[layer_name].mean(dim=0)\n",
    "            other_mean = other_acts[layer_name].mean(dim=0)\n",
    "            \n",
    "            # Steering vector points from \"other\" to \"orange\"\n",
    "            # We'll subtract this to steer away from orange\n",
    "            steering_vector = orange_mean - other_mean\n",
    "            \n",
    "            # Normalize the steering vector\n",
    "            steering_vector = steering_vector / torch.norm(steering_vector)\n",
    "            \n",
    "            steering_vectors[layer_name] = steering_vector\n",
    "            \n",
    "            logger.info(f\"{layer_name}: steering vector norm = {torch.norm(steering_vector):.4f}\")\n",
    "    \n",
    "    return steering_vectors\n",
    "\n",
    "# Compute steering vectors\n",
    "steering_vectors = compute_steering_vectors(orange_activations, other_activations)\n",
    "\n",
    "# Visualize the magnitude of steering vectors across layers\n",
    "layer_numbers = [int(name.split('_')[1]) for name in steering_vectors.keys()]\n",
    "vector_norms = [torch.norm(steering_vectors[name]).item() for name in steering_vectors.keys()]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(layer_numbers, vector_norms, alpha=0.7)\n",
    "plt.xlabel('Layer Number')\n",
    "plt.ylabel('Steering Vector Norm')\n",
    "plt.title('Steering Vector Magnitudes Across Layers')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "console.print(\"‚úÖ Steering vectors computed!\", style=\"bold green\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19282281",
   "metadata": {},
   "source": [
    "## 5. Apply Steering to Model\n",
    "\n",
    "Implement the steering mechanism by modifying the model's forward pass to subtract the steering vector from activations during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21dc10d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SteeringHook:\n",
    "    \"\"\"Hook class to apply steering during forward pass\"\"\"\n",
    "    \n",
    "    def __init__(self, steering_vectors, steering_strength=1.0):\n",
    "        self.steering_vectors = steering_vectors\n",
    "        self.steering_strength = steering_strength\n",
    "        self.active = True\n",
    "    \n",
    "    def get_hook(self, layer_name):\n",
    "        def hook(module, input, output):\n",
    "            if not self.active or layer_name not in self.steering_vectors:\n",
    "                return output\n",
    "            \n",
    "            if isinstance(output, tuple):\n",
    "                hidden_states = output[0]\n",
    "                other_outputs = output[1:]\n",
    "            else:\n",
    "                hidden_states = output\n",
    "                other_outputs = ()\n",
    "            \n",
    "            # Apply steering by subtracting the steering vector\n",
    "            steering_vec = self.steering_vectors[layer_name].to(hidden_states.device)\n",
    "            \n",
    "            # Apply to all positions and batch elements\n",
    "            steered_states = hidden_states - self.steering_strength * steering_vec.unsqueeze(0).unsqueeze(0)\n",
    "            \n",
    "            if other_outputs:\n",
    "                return (steered_states,) + other_outputs\n",
    "            else:\n",
    "                return steered_states\n",
    "        \n",
    "        return hook\n",
    "\n",
    "# Remove old hooks\n",
    "for hook in hooks:\n",
    "    hook.remove()\n",
    "hooks.clear()\n",
    "\n",
    "# Create steering hook\n",
    "steering_hook = SteeringHook(steering_vectors, steering_strength=2.0)\n",
    "\n",
    "# Register new steering hooks\n",
    "for layer_idx in target_layers:\n",
    "    if f\"layer_{layer_idx}\" in steering_vectors:\n",
    "        layer = model.model.layers[layer_idx]\n",
    "        hook = layer.register_forward_hook(steering_hook.get_hook(f\"layer_{layer_idx}\"))\n",
    "        hooks.append(hook)\n",
    "\n",
    "logger.info(f\"Steering hooks registered on {len(hooks)} layers\")\n",
    "console.print(\"‚úÖ Steering mechanism active!\", style=\"bold green\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73c1ad2",
   "metadata": {},
   "source": [
    "## 6. Test Steering Effectiveness\n",
    "\n",
    "Run test prompts that would normally generate \"orange\" and measure how often the steered model avoids outputting this token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c41c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_generation(prompts, label, num_tokens=10, temperature=0.7):\n",
    "    \"\"\"Test model generation with and without steering\"\"\"\n",
    "    results = {\n",
    "        'prompts': [],\n",
    "        'baseline_outputs': [],\n",
    "        'steered_outputs': [],\n",
    "        'baseline_has_orange': [],\n",
    "        'steered_has_orange': [],\n",
    "        'baseline_orange_prob': [],\n",
    "        'steered_orange_prob': []\n",
    "    }\n",
    "    \n",
    "    logger.info(f\"Testing {label} prompts...\")\n",
    "    \n",
    "    for prompt in tqdm(prompts, desc=f\"Testing {label}\"):\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        # Test baseline (without steering)\n",
    "        steering_hook.active = False\n",
    "        with torch.no_grad():\n",
    "            baseline_output = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=num_tokens,\n",
    "                temperature=temperature,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                return_dict_in_generate=True,\n",
    "                output_scores=True\n",
    "            )\n",
    "        \n",
    "        # Test with steering\n",
    "        steering_hook.active = True\n",
    "        with torch.no_grad():\n",
    "            steered_output = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=num_tokens,\n",
    "                temperature=temperature,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                return_dict_in_generate=True,\n",
    "                output_scores=True\n",
    "            )\n",
    "        \n",
    "        # Decode outputs\n",
    "        baseline_text = tokenizer.decode(baseline_output.sequences[0], skip_special_tokens=True)\n",
    "        steered_text = tokenizer.decode(steered_output.sequences[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Check for \"orange\" in outputs\n",
    "        baseline_has_orange = \"orange\" in baseline_text.lower()\n",
    "        steered_has_orange = \"orange\" in steered_text.lower()\n",
    "        \n",
    "        # Get probability of \"orange\" token at first generation step\n",
    "        baseline_probs = torch.softmax(baseline_output.scores[0][0], dim=-1)\n",
    "        steered_probs = torch.softmax(steered_output.scores[0][0], dim=-1)\n",
    "        \n",
    "        baseline_orange_prob = baseline_probs[target_token_id].item()\n",
    "        steered_orange_prob = steered_probs[target_token_id].item()\n",
    "        \n",
    "        # Store results\n",
    "        results['prompts'].append(prompt)\n",
    "        results['baseline_outputs'].append(baseline_text)\n",
    "        results['steered_outputs'].append(steered_text)\n",
    "        results['baseline_has_orange'].append(baseline_has_orange)\n",
    "        results['steered_has_orange'].append(steered_has_orange)\n",
    "        results['baseline_orange_prob'].append(baseline_orange_prob)\n",
    "        results['steered_orange_prob'].append(steered_orange_prob)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test prompts that should naturally lead to \"orange\"\n",
    "test_prompts = [\n",
    "    \"The color of a carrot is\",\n",
    "    \"When you mix red and yellow paint, you get\",\n",
    "    \"A pumpkin is typically colored\",\n",
    "    \"The fruit called an orange is\",\n",
    "    \"Traffic cones are usually painted\",\n",
    "    \"At sunset, the sky often turns\",\n",
    "    \"A basketball is typically\",\n",
    "    \"Tiger fur has black stripes on an\"\n",
    "]\n",
    "\n",
    "# Run tests\n",
    "test_results = test_model_generation(test_prompts, \"orange-prone\", num_tokens=5, temperature=0.3)\n",
    "\n",
    "console.print(\"‚úÖ Testing complete!\", style=\"bold green\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00abdc2f",
   "metadata": {},
   "source": [
    "## 7. Evaluate Results\n",
    "\n",
    "Analyze the steering effectiveness using metrics and visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970e02dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results DataFrame\n",
    "df = pd.DataFrame(test_results)\n",
    "\n",
    "# Calculate summary statistics\n",
    "baseline_orange_rate = df['baseline_has_orange'].mean()\n",
    "steered_orange_rate = df['steered_has_orange'].mean()\n",
    "baseline_avg_prob = df['baseline_orange_prob'].mean()\n",
    "steered_avg_prob = df['steered_orange_prob'].mean()\n",
    "\n",
    "logger.info(\"=== STEERING RESULTS ===\")\n",
    "logger.info(f\"Baseline 'orange' occurrence rate: {baseline_orange_rate:.2%}\")\n",
    "logger.info(f\"Steered 'orange' occurrence rate: {steered_orange_rate:.2%}\")\n",
    "logger.info(f\"Reduction in 'orange' occurrences: {(baseline_orange_rate - steered_orange_rate):.2%}\")\n",
    "logger.info(f\"Baseline avg 'orange' probability: {baseline_avg_prob:.4f}\")\n",
    "logger.info(f\"Steered avg 'orange' probability: {steered_avg_prob:.4f}\")\n",
    "logger.info(f\"Probability reduction: {((baseline_avg_prob - steered_avg_prob) / baseline_avg_prob):.2%}\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. Orange occurrence comparison\n",
    "axes[0,0].bar(['Baseline', 'Steered'], [baseline_orange_rate, steered_orange_rate], \n",
    "              color=['orange', 'blue'], alpha=0.7)\n",
    "axes[0,0].set_ylabel('Orange Occurrence Rate')\n",
    "axes[0,0].set_title('Orange Token Occurrence Rate')\n",
    "axes[0,0].set_ylim(0, 1)\n",
    "\n",
    "# 2. Probability comparison\n",
    "axes[0,1].bar(['Baseline', 'Steered'], [baseline_avg_prob, steered_avg_prob], \n",
    "              color=['orange', 'blue'], alpha=0.7)\n",
    "axes[0,1].set_ylabel('Average Orange Probability')\n",
    "axes[0,1].set_title('Average Orange Token Probability')\n",
    "\n",
    "# 3. Individual prompt comparison\n",
    "prompt_indices = range(len(df))\n",
    "axes[1,0].scatter(prompt_indices, df['baseline_orange_prob'], \n",
    "                  color='orange', alpha=0.7, label='Baseline')\n",
    "axes[1,0].scatter(prompt_indices, df['steered_orange_prob'], \n",
    "                  color='blue', alpha=0.7, label='Steered')\n",
    "axes[1,0].set_xlabel('Prompt Index')\n",
    "axes[1,0].set_ylabel('Orange Probability')\n",
    "axes[1,0].set_title('Orange Probability by Prompt')\n",
    "axes[1,0].legend()\n",
    "\n",
    "# 4. Probability reduction distribution\n",
    "prob_reductions = df['baseline_orange_prob'] - df['steered_orange_prob']\n",
    "axes[1,1].hist(prob_reductions, bins=10, alpha=0.7, color='green')\n",
    "axes[1,1].set_xlabel('Probability Reduction')\n",
    "axes[1,1].set_ylabel('Frequency')\n",
    "axes[1,1].set_title('Distribution of Probability Reductions')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display detailed results\n",
    "console.print(\"\\n[bold]Detailed Results:[/bold]\")\n",
    "for i, row in df.iterrows():\n",
    "    console.print(f\"\\n[bold cyan]Prompt {i+1}:[/bold cyan] {row['prompts']}\")\n",
    "    console.print(f\"[yellow]Baseline:[/yellow] {row['baseline_outputs'][len(row['prompts']):]}\")\n",
    "    console.print(f\"[blue]Steered:[/blue] {row['steered_outputs'][len(row['prompts']):]}\")\n",
    "    console.print(f\"Orange prob: {row['baseline_orange_prob']:.4f} ‚Üí {row['steered_orange_prob']:.4f}\")\n",
    "\n",
    "console.print(\"‚úÖ Analysis complete!\", style=\"bold green\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec63db60",
   "metadata": {},
   "source": [
    "## 8. Cleanup and Summary\n",
    "\n",
    "Clean up resources and provide a summary of the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daaf3e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove hooks to clean up\n",
    "for hook in hooks:\n",
    "    hook.remove()\n",
    "hooks.clear()\n",
    "\n",
    "# Clear GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Summary\n",
    "console.print(\"\\n[bold green]üéØ EXPERIMENT SUMMARY[/bold green]\")\n",
    "console.print(\"=\"*50)\n",
    "console.print(f\"‚úì Successfully loaded Meta Llama 3 8B model\")\n",
    "console.print(f\"‚úì Extracted steering vectors from layers {target_layers}\")\n",
    "console.print(f\"‚úì Applied vector steering to reduce 'orange' token generation\")\n",
    "console.print(f\"‚úì Tested on {len(test_prompts)} prompts\")\n",
    "console.print(f\"‚úì Achieved {((baseline_orange_rate - steered_orange_rate)):.2%} reduction in 'orange' occurrences\")\n",
    "console.print(f\"‚úì Reduced average 'orange' probability by {((baseline_avg_prob - steered_avg_prob) / baseline_avg_prob):.2%}\")\n",
    "\n",
    "logger.info(\"Experiment completed successfully! üöÄ\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
